%Intro-to-intro
%- Computer vision generally
%- augmented reality generally: computers coming to us rather than us coming to computers
%- problem statement
%  - extend competence of visual SLAM to semantically meaningful models
%    - very early work, we did not get all the way there
%    - will focus on scene understanding from multiple views
%  - we will sharpen this definition later in this chapter, then again (formally) in later chapters

Humans are visual creatures and as such our world is built up around
visual cues. The reason we use written street signs rather than, say,
smells or sounds to navigate the road network is that our biological
vision system has both higher fidelity and larger bandwidth than our
other senses. In order for computers to parse our world we have the
choice of imbuing computers with vision--processing systems or
re--signposting (in the most general sense) our world with cues more
amenable to automatic extraction. Computer vision could be seen as the
endeavour to acomplish the former route.

This thesis was originally motivated by the specific application area
of augmented reality (AR). In the broadest possible terms, AR is the
idea of teaching computers to become fluent in the visual experience
of a human, rather than having humans interact with computers on
the computer's terms. The great vision of AR is to have computers
parse, track, and understand the visual field--of--view of a human in
real--time, and seamlessly overlay text and graphics for the benefit
of the wearer.

The problem addressed in this thesis is the development of a computer
vision system that understands the geometry of the environment at a
semantically meaningful level. By ``semantically meaningful'' we mean
at the level of an architectural blueprint or a city roadmap, complete
with annotations and symbols, rather than, say, the level of a set of
points floating in space. Inspired originally by the application of
these ideas to AR we will pay special attention to continuous video
streams (rather than individual frames) and to frame--rate processing
capacity (rather than batch processing). We will refine this problem
statement later in this chapter, then again, quantitatively, in later
chapters.


\section{Motivation}

%Motivations
%- SLAM is not the end of the story for AR
%
%- Semantic structure from motion: using geometry for high-level inference
%- Scene understanding, single-view geometry, and the under-utilization of photometric information for structure recovery

%- context in vision

%- interesting sensors: phone cameras, internet videos, and Kinect and Kinect-like sensors

Over the coarse of the research presented in thesis I have drawn
motivation from a variety of angles. Although they have each pointed
in roughly the same direction, they have shifted my focus between
various parts of the problem.

An early motivation was the extension of visual tracking systems to
high--level reasoning about the visual environment. Simultaneous
localisation and mapping (SLAM) --- the problem of geometrically
accurate mapping of the environment whilst simultaneously locating
oneself within it --- has seen terrific progress over the past several
decades. In particular, visual SLAM, in which the sensing apparatus is
a camera, has taken great strides in the last ten years. The typical
experiments expected of a visual SLAM paper in a top conference have
moved from a single room to an entire city; real--time processing is
now expected rather than impressive; and one can even expect to see
system that reconstruct every pixel in a video stream.

Visual SLAM is a challenging requirement for a convincing AR system,
since to place augmentations contextually within the wearer's
field--of--view, an AR system must first be able to locate itself
pricesly and quickly within the environment. It is neither surprising
nor objectionable, therefore, that so much of the effort towards AR
has focussed on visual SLAM; but now that high--performance SLAM
systems \textit{are} available, the question naturally presents
itself: ``are we done yet?'' Or, in other words, does a ``SLAM
oracle'' that precisely locates the camera at every point in time
completelty solve the AR problem?

The answer I will argue for in this thesis is that no, SLAM is not the
end of the story. While SLAM is certainly a central and important
problem, there remain interesting, challenging, and general computer
vision problems to solve before AR can be fully realised. For example,
the relevance of some piece of information to the wearer varies with
the type of environment within which the wearer is moving. Directions
to a restaurant may be relevant while outdoors but directions to the
kitchen are less likely to be helpful within the wearer's own
house. Within a house, the visual augmentations most likely to be
desirable while in the kitchen are likely to be different to those
suitable to the bedroom. Furthermore, the likely locations of various
objects is tightly coupled to the surfaces and boundaries within an
room. A human would not search for a lost set of keys on the ceiling
first, not outside a window, but to make this inference one must first
identify the orientation of the floor and ceiling and the position of
room boundaries. These problems are not immediately solved by the
ability to locate oneself within an arbitrary 3D coordinate frame ---
though that ability almost certainly helps, as we will show.

Although this thesis was originally motivated from the perspective of
real--time AR, I found much inspiration from the single--view computer
vision community, principally from the literature dedicated to
\textit{scene understanding}, which is the problem of understanding
images in terms of objects, actions, and semantically meaningful
geometric primitives. Scene understanding has a long history within
the single--view computer vision literature but a much shorter history
within the multiple--view and SLAM literature. In the single--view
context, scene understanding is necessarily cast in terms of
photometric sensor data. The question that naturally presented itself
during the development of this research was, ``can we do scene
understanding using multiple views of a scene?''.

The enormity of the scene understanding literature is testament to the
number of different ways that the problem can be cast. Everything from
scene category recognition to semantic segmentation and single--view
reconstruction has been cast within the circle of scene
understanding. The extension of scene understanding to multiple views
opens up perhaps an even broader multitude of approaches, so it would
be foolish to try to present a definitive account of multiple--view
scene understanding within a single thesis. Instead, I have carved off
a piece of the problem that I believe to be large enough to be
interesting, small enough to be tractible, and hopefully insightful
enough to be instructive for future research in this new and exciting
domain. The piece of the puzzle that I have chosen is the extraction
of high--level geometric information within indoor--type environments,
with relevance to scene categorisation and object localisation. I have
worked with both pure photometric models and joint
photometric/geometric models.

%TODO: context in computer vision, semenatic geometry as context

%TODO: put this somewhere? [[There will always be a place for pure computer
%vision. Never--the--less, as new sensors become cheaper and more
%prevalent, it makes sense to use such sensors to the extent that they
%are helpful and cost--effective for the problem at hand.]]

A final motivation that bears mentioning is the exciting acceleration
over the past several years of new platforms suitable for computer
vision applications. Firstly, video on the
internet has become increasingly important over the past several
years, heightening the relevance of computer vision systems that
process video streams rather than single images. It is perhaps
startling to be reminded that the popular internet video website,
YouTube, was founded a mere 6 years ago in 2006, and did not gain
widespread popularity until 2008. Secondly, mobile phones have become
enormously more powerful as computing devices and the cameras now
ubiquitously attached to them have improved in quality by several
orders of magnitude. The reality and relevance of algorithms for
understanding and providing feedback on video streams in real--time
have never been clearer. Finally, depth sensing cameras such as the
Kinect have made an explosive entry into the consumer hardware market
and provide a compelling case for moving beyond

\section{The Need For Geometry}

%The need for geometry
%- experience with photometric-only inference
%  - surprisingly good
%  - to go further, need *coarse*, *semantically helpful* geometry
%- indoor manhattan worlds
%  - simple
%  - good starting point for this new problem
%  - surprisingly broad applicability
%- the value of geometric theorems from an earlier CV period 

We have thus far made the argument that AR needs scene understanding,
but does scene understanding need geometry? Perhaps we could
completely discard structure--from--motion and build AR systems using
the purly photometric approaches of single--view scene
understanding. This seems to me also to be an unreasonably extreme
position. Much amount of work has been dedicatd to understanding
the geometry of cameras and multiple viewpoints; this thesis tries to
show that this work can indeed be leveraged in pursuit of solutions to
semantic--level computer vision problems.

The presentation of this thesis follows, roughly, the chronological
order of the research itself. \Chapref{appearance-only} discusses an
early phase in which I pursued an ``appearance--only'' model for
drawing high--level inferences from video input. While I was impressed
by how far this approach could be pushed, I ultimately decided some
coarse knowledge of the geometric structure of the environment was
needed to answer the semantical--level questions I was pursuing, and
as a result the later chapters are conspicuously geometry--heavy.

The distinction between a ``geometry--less'' and a ``geometry--laden''
approach is of course a soft and conceptual one: In either case one
is, at the end of the day, drawing probabilistic inferences from one
or more input arrays of pixel intensities. The distinction is over of
how those inputs are combined, which conditional independences are
assumed, how a hypothesis class is formulated, and so on.

This thesis is concerned with the recovery of geometry for the sake of
the high--level inference that it enables, not for the sake of the
geometry itself, and our choice of geometric representation reflects
this. We seek a representation which is both suited to semantic--level
scene understanding, and general enough to encompass an interesting
range of environments. A paper by David Lee \cite{Lee09} in 2009
provided the pivotal insight that the so--called ``indoor Manhattan
model'' provided an excellent trade--off between generality and
concision.

Under the indoor Manhattan assumption, the world is composed of a
floor plane, ceiling plane, and a set of vertical walls that meet at
vertical edges. While a strong assumption, this hypothesis class can
represent exactly or approximately a surprisingly broad range of
indoor environments, as well as some outdoor environments dominated by
human structures. Furthermore, the location of the floor, walls, and
ceiling are among the geometric cues most relevant to the
semantic--level scene understanding questions that originally
motivated this thesis. As an illustration of this, consider the image
shown in figure TODO. Despite the low information content of the image
(it consists of just three colours and a set of lines defined by a
mere handful of vertices), it seems that an average human would be
able to make some reasonable inferences about this environment. For
example, we claim that a human would be able to say \textit{something}
in response to the following questions (in order of increasing
uncertainty).
\begin{enumerate}
  \item{What is the direction of gravity?}
  \item{Where would doors most likely be found?}
  \item{Where would a person be most likely to stand?}
  \item{Is this an office or a house?}
  \item{What is the absolute scale of the environment?}
\end{enumerate}
Of course, counter--examples could be constructed to render the answer
to any of these questions arbitrarily erronous. We do not claim that
any of these questions could be answered with certainty, only that the
information in figure TODO would provide non--trivial evidence in
support of answers to the questions above.

\section{Our Approach}

In this thesis we present two approaches to extending the competence
of AR system to semantic--level scene understanding. The first is a
purely photometric approach based on textons. We consider a video
sequence to be a simple time--varying signal, ignoring geometric
constraints. We associate each pixel with a representative texton,
which are exemplars of image patch clusters identified in a training
phase. We show how textons can help answer two semantic--level
questions: scene recognition and object search. Despite the simplicity
of this approach, we show that this appearance--only method is
surprisingly effective for these questions.

To push this approach further, the second part of this thesis shows
how to integrate coarse geometric information including major surfaces
and scene boundaries. We argue that the recently proposed ``indoor
Manhattan model'' provides a compelling trade--off between robustness,
flexibility, and efficiency. Under this model the environment is
represented by a small number of major surfaces such as the floor,
wall, and ceiling of an indoor environment. There is little prior work
on this model (just 2 published conference papers besides our own at
the time of writing), so we dedicate a full chapter to defining the
model formally, working through some simple corollaries, and
discussing two important parametrisations. We then turn to the problem
of probabilistic reasonining within this hypothesis class, which
occupies the final three chapters. In order, we define a probabilistic
model, reduce inference in this model to a particular class of
optimisation problem, solve this optimisation problem using dynamic
programming, then present a discriminative training regime to fix
the free parameters.

The overall approach to the work in this thesis is basic on three
underlying assumptions. First, we believe that probability is the
appropriate language for reasoning from image evidence. As far as
possible we try to present graphical models that make clear our
assumptions before discussing any algorithmic details. Second, we
believe that it is worth leveraging the considerable literature on
camera geometry wherever possible. Finding minimal parametrisations
pays off as robustness and speed of the final algorithm. Finally, we
see computational efficiency as a crucial consideration that rightly
affects both the choice of hypothesis class and the design of
inference algorithms.

Overall
- Bayesian
- ``proof in the pudding'': experiments justify our modelling approximations
  - the hypothesis we test: this model will perform well in practice
    - mind projection falacy?
- separate modelling from inference
  - but choose model judiciously
- relationship between geometry and probability
  - define a hypothesis class using hard geometric constraints
  - connect with sensor data using probabilistic models
  - devise inference algorithms using algorithmic insight
  - mind projection falacy: the wall is not soft, but our estimate of its position is
- inference from ``generative'' standpoint
- learning from ``discriminative'' standpoint
- speed is important
  - cannot really separate performance from speed
  - concentrate on algorithmic complexity not constant-time speedup

\section{Exegesis}

The remainder of this thesis is organised as follows. Chapter 2
presents a review of literature relevant to this thesis. Due to the
connection of our work to the traditionally disparate fields of SLAM
and scene understanding, the literature review breaks down into scene
understanding work that has touched on geometry, and SLAM work that
has touched on scene understanding.

Chapter 3 then presents an appearance--only approach to scene
understanding in the context of a moving camera. We work with textons
as the basic observation--unit and present a probabilistic model
connecting to scene categories and object locations.

We then begin the presentation of our work on indoor Manhattan
models. Chapter 4 presents the model formally, and covers all the
geometric insights necessary for the later chapters.

Chapter 5 switches into the language of probability. We present
graphical models relating sensor data to the geometric primitives we
wish to extract. We give separate probabilistic models for photometric
features, stereo data, and point clouds. We then show how to combine
any combination of these into a joint model, allowing our system to be
easily tailored to many different contexts.

Chapter 6 presents a dynamic programming algorithm that solves the
inference problem for the model presented in chapter 5. It is both
fast and exact, making a compellinge alternative to previous
approaches that fitted neither of these descriptions. We present
compelling comparisons with previous approaches.

Chapter 7 presents a learning routine in which we learn to reconstruct
Manhattan environments from training examples. We cast the learning
problem in a discriminative framework and use state--of--the--art
tools from the structured prediction literature to achieve
polynomial--time training.

Finally, chapter 8 summarises key results, discusses their
ramifications, and suggests directions for future work.
