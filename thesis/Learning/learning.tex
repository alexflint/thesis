%\begin{abstract}
%  We develop a structured prediction approach to reconstructing 3D
%  polygonal models from single and multiple images of a
%  scene. Building on recent advances in single view reconstruction we
%  adopt the indoor Manhattan hypothesis class --- one of the most
%  complicated output spaces (in terms of internal constraints) yet
%  considered within a structured prediction framework. Our approach
%  can learn in both the single-- and multiple--view contexts. We
%  show that the chosen hypothesis class permits optimizing a variety
%  of high--level loss functions, such as the relative depth error. Our
%  results out--perform the state--of--the--art, including an
%  improvement of more than $50\%$ on one metric.
%\end{abstract}

\section{Introduction}
\label{sec:introduction}

This chapter shows how to learn to reconstruct indoor Manhattan models
from training examples. We are given a series of input images together
with ground truth reconstructions provided by hand, and the goal is to
discover and extrapolate this data in order to recover the correct
reconstruction for future images.

Neither the single-- nor multiple--view reconstruction literature
places learning at the heart of the reconstruction problem. Within
single--view reconstruction, for example, few authors cast learning as
a single optimisation with respect to a clearly defined loss function
\cite{Hoiem05,Saxena09,Lee09,Flint11}. On the other hand,
multiple--view reconstruction techniques rarely learn from training
data at all. In contrast, this chapter casts reconstruction
fundamentally as a learning problem, with the goal being to learn a
prediction function $\Predictor$ mapping observed image features to
indoor Manhattan reconstructions.

Using indoor Manhattan reconstructions as the hypothesis class brings
a variety of attractive features such as a simple parametrisation,
efficient and exact inference, and a balance between expressiveness
and robustness, as has been discussed in preceeding chapters. In this
chapter we show that this hypothesis class also leads to a convenient
decomposition of loss functions mirroring the likelihood decomposition
described in \chapref{models}, which permits efficient optimisation of
several popular image--level losses.

For learning we employ the tools of structured prediction, in
particular the structural support vector machine
\cite{Tsochantaridis04}. The use of these tools is part part of a long
trend within computer vision towards statistically rigorous,
well--understood convex optimization techniques. Recent successfully
applications of the structured SVM include detection
\cite{blaschko2008learning}, segmentation \cite{taskar2004max}, and
scene classification \cite{Yang2010}. In the domain of reconstruction,
structured prediction ideas have been applied to several simple models
classes such as stereo disparities \cite{li2008learning} and cuboids
\cite{Hedau09}.

The application of structured prediction to the indoor Manhattan class
of models constitutes one of the most complex output spaces yet
considered within this framework. The indoor Manhattan model enforces
hard geometric constraints that lack simple expressions in terms of
image coordinates. These constraints are context--dependent, being
tied to quantities such as camera rotation and the location of
vanishing points. We have learnt several valuable lessons of general
relevance from this complex prediction task, to which we dedicate the
final section of this chapter.

The contributions of this chapter are thus (i) a unified learning
framework for single-- and multiple--view reconstruction, utilising
the indoor Manhattan model and the tools of structured prediction;
(ii) the reduction of two image--level loss functions to a form
amenable to efficient optimization; (iii) an efficient separation
procedure for identify the ``most--violated constraint'' during
learning, (iv) an empirical demonstration of structured prediction in
perhaps the most complex output space yet considered within this
framework; and (v) a series of practical observations concerning the
application of structured prediction techniques to complicated output
spaces.

%In the remainder of this paper we present background material
%(\secref{background}), followed by our learning framework
%(\secref{learning}). We then present results for multiple--view
%reconstruction (\secref{mv-results}) followed by single--view
%reconstruction (\secref{sv-results}), then we close with practical
%lessons learnt (\secref{discussion}) and concluding remarks
%(\secref{conclusion}).

\section{Background}
\label{sec:background}

\newcommand\inp{\vect{x}}
\newcommand\outp{\vect{y}}

The approach to learning adopted in this chapter has roots in the
structural risk minimization framework developed by Vapnik and
Chervonenkis \cite{Vapnik1998}. On this view, learning is framed in
terms of a predictor $\Predictor$, which maps from an input space
$\mathcal{X}$ to an output space $\mathcal{Y}$. Learning consists of
minimizing the expected loss
\begin{equation}
  \label{eq:expected-utility-maximization}
  \Expected\bigl[ \Loss(f(\inp),\outp) \bigr] 
  =
  \int \Loss\bigl(\Predictor(\inp),\outp\bigr) ~ \intd P(\inp,\outp)
\end{equation}
with respect to the predictor $\Predictor$, where $\Loss$ is a loss
function. Given training samples of the form
$(\inp_i,\outp_i)\in\mathcal{X}\cross\mathcal{Y}$ the integral above is
approximated by a summation
\begin{equation}
  \label{eq:srm}
  \mathcal{R}(\Predictor) + \sum_i \Loss\bigl(\Predictor(\inp_i),\outp_i\bigr)
\end{equation}
where $\mathcal{R}(\Predictor)$ is a regularizer on
$\Predictor$. Structural risk minimization is very general and does
not specify any particular form for the input space, output space,
predictor class, or loss function. Many machine learning algorithms
can be viewed as instantiations of structural risk minimization for
particular predictor classes and loss functions. For example, the
classic support vector machine (SVM) \cite{Cortes1995} is defined for
Hilbert input spaces, binary outputs, the class of linear predictors,
and the Hinge loss function. In this case equation \eqnref{srm}
becomes
\begin{equation}
  \|\Model\|^2 + \sum h(\outp_i, \Model \cdot \inp_i))
\end{equation}
where $h$ is the hinge loss, and learning consists of minimization
with respect to $\Model$. Rewriting the hinge loss terms as constraints leads
to the well--known quadratic programming formulation of the SVM
\cite{Cortes1995}.

Extending binary classification to more general output spaces is a
major research programme within the machine learning community; an
excellent introduction to recent advances is given by Bakir \etal
\cite{Bakir07}. One recently successful approach is the structured
support vector machine proposed by Tsochantaridis \etal
\cite{Tsochantaridis04}, which reduces learning in non--binary
(``structured'') output spaces to a ranking problem. The structured
SVM makes use of a joint feature space $\JointFtr(\inp,\outp)$ mapping
input/output pairs to real--valued vectors. The structured SVM
considers predictors of the form
\begin{equation}
  \Predictor(\inp;\Model) 
  = 
  \argmax_{\outp}\limits 
  \bigl\langle \Model, \JointFtr(\inp,\outp) \bigr\rangle
  \label{eq:linear-max}
\end{equation}
where $\Model$ is a parameter vector and $\langle\cdot,\cdot\rangle$
denotes an inner product. As in the classic SVM, the hinge loss is
used to define the objective function,
\begin{equation}
  \label{eq:first-ssvm}
  \|\Model\|^2 + \sum h(\outp_i, \Predictor(\inp_i)) ~.
\end{equation}
In order to optimize with respect to $\Model$, Toschantaridis \etal
showed how to write \eqnref{first-ssvm} as a constrained optimization
problem parallelling the classic SVM:
\begin{equation}
  \begin{split}
    \min_{\Model,\Slacks} &
      \hspace{2mm} 
    \frac{1}{2} \|\Model\|^2 +
      C \sum_{i=1}^n \Slack_i\\
    s.t. & \hspace{2mm} 
      \forall k, \outp \neq \outp_i:~
        \bigl\langle\Model, \JointFtr(\inp_i,\outp_i)\bigr\rangle -
        \bigl\langle\Model, \JointFtr(\inp_i,\outp)\bigr\rangle
      \geq
        \Loss(\outp,\outp_i) - \Slack_i ~.
  \end{split}
  \label{eq:ssvm}
\end{equation}
Unfortunately the number of constraints in \eqnref{ssvm} is in general
very large since there is a constraint for every element of the output
space. Tsochantaridis \etal \cite{Tsochantaridis04} showed how to
overcome this by exploiting the sparsity structure of support vector
machines. Their algorithm iteratively adds constraints beginning with
zero constraints, and they showed that only a polynomial number of
constraints need ever be added in order to achieve an
$\epsilon$--accurate solution.

The remainder of this chapter shows how to use the structured SVM to
learn to reconstruct indoor Manhattan models.

\section{Model}
\label{sec:model}

In this section we describe three components of the model that we are
trying to learn (and, at test time, infer): a hypothesis class, a
feature space, and a loss function. In our setup these are,
respectively, the class of indoor Manhattan models, a log--linear
Bayesian likelihood, and either the relative depth error or a
labelling error (we describe both).

\subsection{Hypothesis Class}

This chapter is concerned with the output space consisting of indoor
Manhattan reconstructions as described in \chapref{geometry}. For
consistency with preceeding chapters we use the following notation:
our input space consists of observed image features denoted
$\Features$, and the output space consists of indoor Manhattan
reconstructions in the seam representation denoted $\Seam$. A training
sample is then a set of pairs $\{(\Features_i,\Seam_i)\}$.

\subsection{Feature Space}

In order to learn to reconstruct indoor Manhattan models we must
define a family of prediction functions $\Predictor$ mapping image
features to indoor Manhattan models. Building on \chapref{inference},
we would like each predictor to implement MAP inference for some
value of the hyper--parameters, \ie we are interested in predictors of
the form
\begin{equation}
  \label{eq:map-predictor}
  \Predictor(\Features) = 
  \argmax_{\Seam}\limits P(\Seam ~|~ \Features,\Model)
\end{equation}
for various $\Model\in\Reals^n$. In other words, we would like to
optimise over the set
\begin{equation}
  \Bigl\{ 
    \Predictor ~\bigl|~
    \Predictor: \Features \longmapsto 
    \argmax_{\Seam}\limits P(\Seam ~|~ \Features,\Model), \Model\in\Reals^n
  \Bigr\}
\end{equation}
consisting of MAP inference predictors for all possible
hyper--parameter values, which we have grouped into the parameter
$\Model$. In order to employ the structured SVM we need to write
$\Predictor$ in the form \eqnref{linear-max}, or equivalently, we need
to write the log--posterior $\log P(\Seam ~|~ \Features,\Model)$ in the form
\begin{equation}
  \log P(\Seam ~|~ \Features,\Model)
  =
  \bigl\langle \Model, \JointFtr(\Features,\Seam) \bigr\rangle ~.
\end{equation}
The remainder of this section is devoted to defining a joint feature
map $\JointFtr$ permitting such a representation. We now examine each
term in the log--posterior \eqnref{full-posterior}, our goal being to
show that each is linear in the relevant hyper--parameters.

\subsubsection{Prior}
Recall the log--prior \eqnref{log-scene-prior}, which is repeated
below.
\begin{equation}
  \label{eq:log-scene-prior2}
  \log P(\Seam ~|~ \Penalties) =
    -\log Z +
    n_1\log\PenaltyConc + 
    n_2\log\PenaltyConv + 
    n_3\log\PenaltyOccl~.
\end{equation}
Defining $\Counts=\begin{bmatrix}n_1&n_2&n_3\end{bmatrix}$ we have
\begin{equation}
  \label{eq:linear-prior}
  \log P(\Seam ~|~ \Penalties) 
  =
  \bigl\langle \Penalties, \Counts \bigr\rangle ~~ + O(1) ~.
\end{equation}

\subsubsection{Photometric Likelihood}
Combining \eqnref{photometric-loglik} and \eqnref{pixel-likelihood},
the log likelihood for photometric features that was described in
\chapref{models} is
\begin{equation}
  \log P(\Features ~|~ \Seam) 
  =
  \sum_i \PixelModel_\Orient \cdot \Feature ~~ + O(1) ~.
\end{equation}
Next we define
\begin{align}
  \PixelModel =
  \begin{bmatrix}
    \PixelModel_1 \\
    \PixelModel_2 \\
    \PixelModel_3
  \end{bmatrix}
  & \quad &  
  \JointFtr_{\textsf{mono}}(\Features,\Seam) =
  \begin{bmatrix}
    \sum_{i:\PredictedOrient_i=1}\limits \Feature_i \vspace{1mm} \\
    \sum_{i:\PredictedOrient_i=2}\limits \Feature_i \vspace{1mm} \\
    \sum_{i:\PredictedOrient_i=3}\limits \Feature_i
  \end{bmatrix}~.
\end{align}
Finally we can write
\begin{equation}
  \label{eq:linear-photometric-model}
  \log P(\Features ~|~ \Seam) 
  =
  \bigl\langle \PixelModel, \JointFtr_{\textsf{mono}}(\Features,\Seam)
  \bigr\rangle ~~ + O(1) ~.
\end{equation}

\subsubsection{Photoconsistency Likelihood}
The log likelihood for photoconsistency features given by equation
\eqnref{stereo-loglik} is
\begin{equation}
  \label{eq:stereo-loglik2}
  \log P(\Features,\AuxFeatures_1,\ldots,\AuxFeatures_{\NumViews}
            ~|~ \Seam) 
  =
  \sum_i \sum_{k=0}^{\NumViews} 
    \|\Feature_i-\ComputedAuxFeature_k(\Pixel_i;\Seam)\|_{\StereoCov} ~.
\end{equation}
Here we assume that $\StereoModel$ has the diagonal form
\begin{equation}
  \StereoCov^{-1} = \diag(\StereoCoef_1,\ldots,\StereoCoef_2) ~.
\end{equation}
It follows that by defining
\begin{align}
  \StereoModel =
  \begin{bmatrix}
    \StereoCoef_1 \\
    \vdots \\
    \StereoCoef_m
  \end{bmatrix}
  & \quad &  
  \JointFtr_{\textsf{stereo}}(\Features,\Seam) =
  \begin{bmatrix}
    \sum_i \sum_{k=1}^K \bigl(
      (\Feature_i)_1 - (\ComputedAuxFeature_k(\Pixel_i;\Seam))_1
    \bigr)^2\\
    \vdots \\
    \sum_i \sum_{k=1}^K \bigl(
      (\Feature_i)_m - (\ComputedAuxFeature_k(\Pixel_i;\Seam))_m
    \bigr)^2
  \end{bmatrix}~.
\end{align}
we can write the log likelihood \eqnref{stereo-loglik2} in the form
\begin{equation}
  \label{eq:linear-stereo-model}
  \log P(\Features,\AuxFeatures_1,\ldots,\AuxFeatures_{\NumViews})
  =
  \bigl\langle
    \StereoModel, \JointFtr_{\textsf{stereo}}(\Features,\Seam)
  \bigr\rangle ~~ + O(1) ~.
\end{equation}

\subsection{Point Cloud Likelihood}

The log--likelihood for point cloud features \eqnref{3d-likelihood} is
non--linear so for the purpose of learning we approximate it by a
first order Taylor expansion about $\IndModelo$ as follows.
\begin{eqnarray}
  \log P(\Depth~|~\Seam,\IndModel)
  &\approx&
  \frac
      {\sum_{\Ind} P(\Depth~|~\Seam,\Ind) P(\Ind~|~\IndModel)}
      {\sum_{\Ind} P(\Depth~|~\Seam,\Ind) P(\Ind~|~\IndModelo)} + O(1)\\
  \log P(\Depths~|~\Seam,\IndModel)
  &\approx&
  \sum_i \sum_{\Ind} \frac
      {P(\Depth_i~|~\Seam,\Ind) P(\Ind~|~\IndModel)}
      {\sum_{\Ind} P(\Depth_i~|~\Seam,\Ind_i) P(\Ind~|~\IndModelo)} + O(1) ~.
\end{eqnarray}
It follows that by defining
\begin{equation}
  \JointFtr_{\textsf{3D}}(\Depths,\Seam) =
  \frac{1}{\sum_{\Ind} P(\Depth_i~|~\Seam,\Ind) P(\Ind~|~\IndModelo)}
  \begin{bmatrix}
    \sum_i P(\Depth_i~|~\Seam,\Ind=\IN) P(\Ind=\IN~|~\IndModel) \\
    \sum_i P(\Depth_i~|~\Seam,\Ind=\OUT) P(\Ind=\OUT~|~\IndModel) \\
    \sum_i P(\Depth_i~|~\Seam,\Ind=\ON) P(\Ind=\ON~|~\IndModel)
  \end{bmatrix}~.
\end{equation}
we can write
\begin{equation}
  \label{eq:linear-pt-model}
  \log P(\Depths~|~\Seam,\IndModel)
  =
  \bigl\langle
    \IndModel, \JointFtr_{\textsf{3D}}(\Depths,\Seam)
  \bigr\rangle ~~ + O(1) ~.
\end{equation}

\subsubsection{Posterior}

We have now shown that the likelihoods we defined in \chapref{models}
for each sensor modality are log--linear in the respective
hyper--parameters. Combining equations \eqnref{eq:linear-prior},
\eqnref{eq:linear-photometric-model}, \eqnref{eq:linear-stereo-model},
and \eqnref{eq:linear-pt-prior} we have the following linear form for
the posterior,
\begin{equation}
  P(\Seam ~|~ X, \Model)
  =
  \bigl\langle
    \Model, \JointFtr(\Observations,\Seam)
  \bigr\rangle
\end{equation}
where $\Observations$ denotes all observations from all sensor
modalitities and we have defined
\begin{align}
  \Model = \begin{bmatrix}
    \Penalties \\
    \PixelModel \\
    \StereoModel \\
    \IndModel
  \end{bmatrix}
  & \quad &
  \JointFtr(\Observations,\Seam) = \begin{bmatrix}
    \Counts \\
    \JointFtr_{\textsf{mono}} \\
    \JointFtr_{\textsf{stereo}} \\
    \JointFtr_{\textsf{3D}}
  \end{bmatrix}~.
\end{align}

\subsubsection{Discussion}

We have shown in the derivations above that the posterior from chapter
\sectref{models} is log--linear (or can be approximated as such) in
the hyper--parameters, so substituting \eqnref{linear-posterior} into
\eqnref{map-predictor} we are now interested in predictors of the form
\begin{equation}
  \label{eq:linear-predictor}
  \Predictor(\Features) = 
  \argmax_{\Seam}\limits
  \bigl\langle
    \Model, \JointFtr(\Observations,\Seam)
  \bigr\rangle
\end{equation}

One view of this result is that it is a convenient property of our
chosen likelihoods that allows us to efficiently optimise the
hyper--parameters in the graphical models of \chapref{models} using
learning algorithm that we describe below. However, an alternative
view is that the joint feature map is simply an aribtrary feature
space with no special interpretation attached to any element. Each
element of the feature space simply expands the range of predictors
that can be expressed in the form \eqnref{linear-predictor}, and we
may add whichever features we believe will be helpful for
reconstruction. The fact that our features are derived from a
graphical model is suggestive of their relevance but not absolutely
necessary.

Both views lead to the same learning objective, which is to optimise
$\Model$ with respect to an expected loss over a training set
$\{(\Features_i,\Seam_i)\}$.

\subsubsection{Features}

The precise make--up of the feature space depends on the available
sensor modalities. For our experiments we define separate feature
spaces for the single-- and multiple--view contexts; these are
summarized in \figref{featurespace}.

\begin{figure}[tb]
  \centering
  \begin{tabular}{@{}lllll@{}}
    \toprule
    Feature & Dimensionality & Multi--view? & Single--view? & Reference \\
    \midrule
    Stereo photo--consistency & 4 & yes & no & Flint \etal \cite{Flint11}\\
    Point cloud & 2 & yes & no & Flint \etal \cite{Flint11}\\
    Line sweeps & 1 & yes & yes & Lee \etal \cite{Lee09}\\
    RGB+HSV & 6 & no & yes & \\
    Gabor responses\footnotemark & 12 & no & yes & \\
    \bottomrule
  \end{tabular}
  \caption{The composition of our single-- and multiple--view feature
    space. We omit color and Gabor features from the multiple--view
    feature space for training efficiency.}
  \label{fig:featurespace}
\end{figure}

\footnotetext{4 orientations, 3 scales}

\subsection{Loss Functions}

In this section we define a loss function $\Delta(\Seam,\EstSeam)$
measuring the cost of predicting some reconstruction $\EstSeam$ when
in fact the true reconstruction is $\Seam$. In the context of learning
one often faces a trade--off between choosing a loss that leads to
tractable optimization, and choosing a loss that measures the quantity
that one ``really'' cares about. For example, Hoiem \etal
\cite{Hoiem05} learn a per--segment orientation classifier, then pass
this as input to a separate 3D reconstruction system
\cite{Hoiem2005}. However, what one ``really'' cares about is some
loss defined on the output of the entire system rather than the output
of individual components, since some segment--level mistakes are
insignificant to the overall reconstruction quality, while others are
catastrophic. This is not a criticism of the authors' choice, but an
illustration of the trade--off faced when choosing a loss function. In
this paper we show how to learn efficiently with respect to loss
functions defined on the final reconstruction.

The relative depth error has been the gold standard within the
reconstruction community for more than a decade \cite{Hartley04}, and
measures the average deviation between reconstructed and ground truth
depths. In our notation,
\begin{equation}
  \label{eq:depth-loss}
  \DepthLoss(\Seam,\EstSeam)
  =
  \frac{1}{N}
  \sum_{\Pixel}
  \frac
      {\abs{\Depth(\Pixel;\EstSeam) - \Depth(\Pixel;\Seam)}}
      {\Depth(\Pixel;\Seam)} ~,
\end{equation}
where $N$ is the number of pixels. Another reasonable choice is the
labelling error, used widely within the semantic segmentation
literature,
\begin{equation}
  \label{eq:label-loss}
  \LblLoss(\EstSeam,\Seam)
  =
  \frac{1}{N}
  \sum_{\Pixel}
  \bigl[~\Orient(\Pixel;\EstSeam) \neq \Orient(\Pixel;\Seam)~\bigr] ~,
\end{equation}
where $[p]$ is 1 if $p$ is true and 0 otherwise. An attractive
characteristic of the indoor Manhattan class is that \textit{both of
  these losses can be optimized exactly}. The algorithmic details are
left to \sectref{learning}; the key result we establish here is that
$\DepthLoss$ and $\LblLoss$ can be written in a form resembling the
payoff formulation \eqnref{likelihood} for the feature likelihoods.

First we invoke the independence established in \eqnref{depth-indep}:
\begin{equation}
  \DepthLoss(\EstSeam,\Seam)
  =
  \frac{1}{N}
  \sum_{x=1}^{\Width}
  \sum_{y=1}^{\Height}
  \frac
      {\abs{\tilde{\Depth}(x,y;\hat{\seam}_x) - \Depth(x,y;\Seam)}}
      {\Depth(x,y;\Seam)} ~.
\end{equation}
Defining a real matrix $\LossMat_{\Seam}$,
\begin{equation}
  \LossMat_{\Seam}(x,j) =
  \sum_{y=1}^H
  \frac
      {\abs{\tilde{\Depth}(x,y;j) - \Depth(x,y;\Seam)}}
      {\Depth(x,y;\Seam)} ~,
\end{equation}
we see that we can write $\DepthLoss$ in the form
\begin{equation}
  \DepthLoss(\EstSeam,\Seam)
  =
  \frac{1}{N}
  \sum_{x=1}^{\Width} \LossMat_{\Seam}(x,\hat{\seam}_x) ~.
  \label{eq:depth-loss-matrix}
\end{equation}
There is a similar form for $\LblLoss$, which we omit here due to
space constraints.

\subsubsection{Choosing a Loss Function}

Neither of the above losses is unequivocally the ``correct'' loss; the
choice will depend on the application. One might expect a strong
correlation between the losses, and indeed one can show analytically
that
\begin{equation}
  \DepthLoss(\EstSeam,\Seam) = 0
  \iff
  \LblLoss(\EstSeam,\Seam) = 0 ~.
\end{equation}
However, in our experiments we found only a weak correlation between
these losses away from the origin. For example, the scatter plot shown
in figure \figref{comparative-scatter} shows a significant number of
outliers that score very well on $\DepthLoss$ but poorly on
$\LblLoss$, and vice versa.

\section{Learning Algorithm}
\label{sec:learning}

We turn now to the problem of learning within the model described
above. Our learning task is to identify a prediction function
$\Predictor$ mapping observed features $\Features$ to reconstructions
$\Seam$. We seek the loss minimizer
\begin{equation}
  \OptPredictor = 
  \argmin_{\Predictor} 
  \Expected \Bigl[ 
    \Loss\bigl(\Predictor(\Features),\Seam\bigr)
    \Bigr] ~,
\end{equation}
which we approximate in the framework of structural risk minimization as
\begin{equation}
  \OptPredictor = 
  \argmin_{\Predictor} 
    \mathcal{R}(\Predictor) \sum_k
      \Loss\bigl(\Predictor(\Features_k),\Seam_k\bigr) ~,
\end{equation}
where $k$ indexes a training set and $\mathcal{R}$ is a
regularizer. To perform this optimization we turn to the tools of
structured prediction \cite{Bakir07}, and in particular the structured
SVM \cite{Tsochantaridis04}. Following the standard approach
\cite{Bakir07} we cast the learning problem as a constrained
optimisation problem,
\begin{equation}
  \begin{split}
    \min_{\Model,\Slacks} &
      \hspace{2mm} 
    \frac{1}{2} \|\Model\|^2 +
      C \sum_{k=1}^n \Slack_k\\
    s.t. & \hspace{2mm} \forall k, \Seam \neq \Seam_k:~
      \bigl\langle\Model, \JointFtr(\Features_k,\Seam_k)\bigr\rangle -
      \bigl\langle\Model, \JointFtr(\Features_k,\Seam)\bigr\rangle
      \geq
      \Loss(\Seam,\Seam_k) - \Slack_k ~.
  \end{split}
  \label{eq:svm-problem}
\end{equation}
where $C$ is a user--specified parameter that trades off between
prediction accuracy and model complexity. Tsochantaridis \etal
\cite{Tsochantaridis04} described an algorithm for solving this
minimisation that is now used extensively within machine learning and
computer vision. To apply this algorithm here we must solve two
inference problems:
\begin{enumerate}
  \item{\textit{Prediction.} This is the maximization described in
    \eqnref{linear-predictor}.}
  \item{\textit{Separation.} The algorithm described in
    \cite{Tsochantaridis04} requires a user--supplied procedure to
    find the ``most--violated constraint'' at each iteration. That is,
    \begin{equation}
      \argmax_{\Seam}
        \bigl\langle\Model, \JointFtr(\Features_k,\Seam)\bigr\rangle
        + \Loss(\Seam,\Seam_k) ~.
        \label{eq:separation-problem}
    \end{equation}
  }
\end{enumerate}

Our solutions to both of the above build on the dynamic programming
algorithm presented in \chapref{inference}, which solves problems of
the form
\begin{equation}
  \argmax_{\Seam}
    \sum_x \PixelPayoff(x,\seam_x) - \sum_j \CornerPenalty(j;\Seam) ~.
  \label{eq:inference-problem}
\end{equation}

\subsection{Inference (Prediction)}

We showed in \secref{model} that \eqnref{inference-problem} can be
written in the form \eqnref{linear-predictor}, so the prediction
problem is a straightforward application of the inference algorithm of
\chapref{inference}. This is as expected since, as we have already
remarked, \eqnref{linear-predictor} is equivalent to MAP inference on
indoor Manhattan reconstructions, which was precisely the subject of
\chapref{inference}.

\subsection{Loss--Augmented Inference (Separation)}

The separation problem, it turns out, can also be solved using the
dynamic programming algorithm from \chapref{inference}, as the
following proposition shows.

\begin{proposition}
  Let $(\Features_k,\Seam_k)$ be a training instance with payoff
  matrices $\{\PixelPayoff_i\}$ as defined in \eqnref{likelihood}.
  Let
  \begin{equation}
    \AugPayoff = \LossMat_{\Seam_k} + \sum_i \PixelPayoff_i ~.
    \label{eq:aug-payoff}
  \end{equation}
  Then the solution to \eqnref{inference-problem} with
  $\PixelPayoff=\AugPayoff$ is identical to the solution to
  \eqnref{separation-problem}.
\end{proposition}
\begin{proof}
  Direct equivalence of the expressions to be maximized. First
  substitute \eqnref{log-linear-posterior} and
  \eqnref{depth-loss-matrix} into \eqnref{inference-problem}:
  \begin{equation}
    \log P(\Seam ~|~ \Features) +
    \sum_{x=1}^{\Width} \LossMat_{\Seam_k}(x,\seam_x) ~
  \end{equation}
  Further substituting \eqnref{likelihood} and defining
  $\CornerPenalty$ as in \cite{Flint11} gives
  \begin{equation}
    \sum_i \sum_{x=1}^{\Width} \PixelPayoff_i(x,\seam_x) - 
    \sum_j \CornerPenalty(j;\Seam) +
    \sum_{x=1}^{\Width} \LossMat_{\Seam_k}(x,\seam_x) ~.
  \end{equation}
  Finally we see that substituting \eqnref{aug-payoff} gives
  \begin{equation}
    \sum_{x=1}^{\Width} \AugPayoff(x,\seam_x) - 
    \sum_j \CornerPenalty(j;\Seam)
  \end{equation}  
\end{proof}

\section{Multiple View Results}
\label{sec:mv-results}

We evaluated our approach on the data--set proposed in \cite{Flint11},
which consists of 18 sequences of six environments averaging 59
seconds in duration. We sampled key--frames at regular intervals. Each
``instance'' in our training and hold--out sets consists of one base
frame together with four auxiliary frames.

We compared with the bootstrapping approach described in
\cite{Flint11}. Our metrics differ from theirs in two ways. Firstly,
they compute relative depth error using the maximum of the ground
truth and estimated depths in the denominator, whereas we always use
the ground truth in the denominator. These metrics are separated by at
most a monotonic transform but the latter is more convenient to
represent in our framework. Secondly, when we compute labelling error
we differentiate vertical and horizontal surfaces only, whereas they
also differentiate the two vertical orientations. The latter approach
makes a side--by--side comparison difficult because the two vertical
orientations are symmetric and their labels can always be
interchanged.

The performance of these two algorithms are summarized in
\figref{mv-performance}. Our approach significantly out--performs the
bootstrapping algorithm. Anecdotally we noticed that much of the
improvement resulted from a reduction in catastrophic failures. This
makes sense because we would expect the learning algorithm to
concentrate on reducing those mistakes that result in the largest
loss. Some example predictions are shown in \figref{results-pics};
many more are included in additional material.

\begin{figure}[tb]
  \centering
  \begin{tabular}{@{}p{20mm}p{20mm}p{20mm}p{20mm}p{20mm}@{}}
    \toprule
     & \multicolumn{2}{c}{Depth Error (\%)}
     & \multicolumn{2}{c}{Labelling Error (\%)} \\
    \midrule
    Sequence & This Paper\footnotemark[2] & Flint \etal 
             & This Paper\footnotemark[3] & Flint \etal \\
    \midrule
    \tt{ground}   & 4.9    & 66.6    & 2.9    & 10.4  \\
    \tt{foyer1}   & 6.1    & 6.6     & 3.1    & 3.1   \\
    \tt{foyer2}   & 4.3    & 5.4     & 3.7    & 4.0   \\
    \tt{corridor} & 14.6   & 52.9    & 9.5    & 19.2  \\
    \tt{mcr}      & 34.0   & 67.6    & 15.    & 16.2  \\
    \tt{kitchen}  & 16.8   & 23.6    & 5.2    & 6.1   \\
    \midrule
    Average       & \textbf{13.4} & 37.1 & \textbf{6.7} & 9.8   \\
    \bottomrule
  \end{tabular}
  \caption{Multiple--view reconstruction performance on held--out
    data, comparing with Flint \etal \cite{Flint11}. For unavoidable
    reasons we use slightly different metrics so our figures differ
    from those published in \cite{Flint11}. See main text for
    explanation.}
  \label{fig:mv-performance}
\end{figure}

\begin{figure}[tb]
  \centering
  \begin{tabular}{@{}p{20mm}p{20mm}p{20mm}p{20mm}p{20mm}@{}}
    \toprule
     & \multicolumn{2}{c}{Depth Error (\%)}
     & \multicolumn{2}{c}{Labelling Error (\%)} \\
    \midrule
    Sequence & This Paper\footnotemark[2] & Flint \etal 
             & This Paper\footnotemark[3] & Flint \etal \\
    \midrule
    \tt{ground}   & 17.3   & 24.5    & 7.8    & 12.2  \\
    \tt{foyer1}   & 25.1   & 31.0    & 15.1   & 22.2  \\
    \tt{foyer2}   & 29.1   & 30.1    & 15.9   & 18.6  \\
    \tt{corridor} & 31.7   & 33.6    & 19.3   & 24.8  \\
    \tt{mcr}      & 70.1   & 45.9    & 26.7   & 20.8  \\
    \tt{kitchen}  & 25.1   & 26.2    & 7.7    & 11.9  \\
    \midrule
    Average       & 33.1   & \textbf{31.9} & \textbf{15.4} & 18.4   \\
    \bottomrule
  \end{tabular}
  \caption{Single--view reconstruction performance on held--out data,
    comparing with Flint \etal \cite{Flint10eccv}.}
  \label{fig:sv-performance}
\end{figure}


\begin{figure}[tb]%
  \centering
  \includegraphics[width=0.6\textwidth]{comparative_scatter}
  \caption{The effect of the loss function on training. We train two
    predictors, one with respect to $\DepthLoss$ and one with respect
    to $\LblLoss$, then evaluate both on all held--out instances. Each
    data point shows the error obtained by one predictor on held--out
    instance. The differing distribution of errors shows that the two
    predictors trade off errors as expected.}
  \label{fig:comparative-scatter}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{psi_evolution}
  \caption{Evolution of $\Model$ during training. Each series shows
    the value of one component of $\Model$. After an
    exploration phase the model converges.}
  \label{fig:psi-evolution}
\end{figure}

\section{Single View Results}
\label{sec:sv-results}

We evaluated our system for single--view reconstruction using the same
data--set described in the previous section. We used the single--view
features summarized in \figref{featurespace}. We compared our approach
to the single--view approach of Flint \etal \cite{Flint10eccv}, which
uses the same dynamic programming algorithm that we rely upon, but
uses hand--tuned features.

Performance for each algorithm is summarized in
\figref{mv-performance}. When measured by labelling error, our
approach out--performs the hand--tuned weights, but on the depth error
metric our approach is inferior. While investigating this result we
found that our learning algorithm assigns small weights to all but the
line--sweep features, which are the same features used by
\cite{Flint10eccv}. This suggests that the hand--tuned weights are in
fact close to optimal within this feature space, though one would
expect that with additional feature engineering our learning algorithm
would be able to leverage further salient information and reduce the
error rate.

\footnotetext[2]{This column represents the predictor trained with
  respect to $\DepthLoss$.}

\footnotetext[3]{This column represents the predictor trained with
  respect to $\LblLoss$.}

\section{Discussion}
\label{sec:discussion}

The hypothesis class considered in this paper is amongst the most
complex (in terms of internal constraints on the output space) studied
within the structured prediction framework. In this section we turn to
some practical lessons learnt that may be of value to other
practitioners.

\subsubsection{Condition in the joint feature space, not the input
  feature space.}
gi
A common pre--processing operation for statistical learning is to
transform the observed features $\Features$ to zero mean and unit
variance. However, for structured prediction tasks it is the joint
feature space $\JointFtr$ that should be conditioned:
\begin{equation}
  \label{eq:conditioning}
  \JointFtr' = \frac{\JointFtr - \mu}{\sigma^2} ~.
\end{equation}
Ideally one would sample from the joint feature space to determine the
conditioning transformation, but the distribution of inputs and
outputs is generally unknown in an empirical risk minimization
setting. Instead, we use the training set as a proxy. We compute the
empirical mean and variance of
$\{\JointFtr(\Features_k,\Seam_k)\}_{k=1}^n$ at the outset, then apply
the transformation \eqnref{conditioning} after each feature
computation.

\subsubsection{Condition the loss terms.}

For any $\eta>0$, the minimization problem \eqnref{svm-problem} is
equivalent (under the substitution $\Model'=\eta\Model$,
$\Slacks'=\eta\Slacks$) to:
\begin{equation}
  \begin{split}
    \min_{\Model',\Slacks'} &
      \hspace{2mm} 
    \frac{1}{2} \|\Model'\|^2 +
      \eta C \sum_{k=1}^n \Slack_k'\\
    s.t. & \hspace{2mm} \forall k, \Seam \neq \Seam_k:~
      \bigl\langle\Model', \JointFtr(\Features_k,\Seam_k)\bigr\rangle -
      \bigl\langle\Model', \JointFtr(\Features_k,\Seam)\bigr\rangle
      \geq
      %%% TODO: in the line below 1/eta should be replaced with eta --
      %%% this was incorrect in the review submission!!
      \eta \Loss(\Seam,\Seam_k) - \Slack_k' ~.
  \end{split}
  \label{eq:equiv-problem}
\end{equation}
Although any $\eta>0$ preserves the correctness of the optimization
algorithm, we found that choosing $\eta=\mbox{Var}(\Loss)$ improved
numerical stability, since this means the loss terms will have roughly
unit variance. Unfortunately, we cannot use the training set to
estimate $\mbox{Var}(\Loss)$ since the loss for the ground truth
reconstruction is always zero. Instead we computed
$\Loss(\Features_k,\Seam_j)$ for each $k \neq j$ in the training
set. This is not an ideal estimate, but we found that it worked well
in practice.

\subsubsection{Check that the hypothesis class contains the ground
  truth.}

The algorithm described in \cite{Tsochantaridis04} implicitly assumes
that the hypothesis class $\mathcal{Y}$ contains the ground truth
labels $\Seam_k$. This means that if $\AugSeam$ is the maximizer of
\eqnref{separation-problem} then
\begin{equation}
  \bigl\langle\Model, \JointFtr(\Features_k,\Seam_k)\bigr\rangle
  - \bigl\langle\Model, \JointFtr(\Features_k,\AugSeam)\bigr\rangle
  - \Loss(\AugSeam,\Seam_k) 
  \leq 
  0 ~,
  \label{eq:positive-slacks}
\end{equation}
since otherwise we would have
\begin{equation}
  \bigl\langle\Model, \JointFtr(\Features_k,\Seam_k)\bigr\rangle
  + \Loss(\Seam_k,\Seam_k) 
  >
  \bigl\langle\Model, \JointFtr(\Features_k,\AugSeam)\bigr\rangle
  + \Loss(\AugSeam,\Seam_k) ~,
\end{equation}
contradicting $\AugSeam$ as the maximizer of
\eqnref{separation-problem}. However, our output space contains
fundamentally real--valued quantities such as polygon vertices, which
are recovered only to some finite precision by the inference
algorithm, and since our ground truth labels were acquired by manual
labelling, they sometimes exceed the maximum precision of the
inference algorithm. In this case we effectively have $\Seam_k \notin
\mathcal{Y}$ (although there is always some $\Seam' \in \mathcal{Y}$
close to $\Seam_k$), so it is possible that $\AugSeam$ violates
\eqnref{positive-slacks}. Our workaround here is simply to check the
condition \eqnref{positive-slacks} each time we solve the separation
problem and, if violated, substitute $\Seam_k$ for $\AugSeam$. This is
justified by the observation that if \eqnref{positive-slacks} is
violated for $\AugSeam$ then it is violated for all
$\Seam\in\mathcal{Y}$. One could think of this as learning with
respect to the hypothesis class $\mathcal{Y} \union \{\Seam_k\}$ but
evaluating with respect to $\mathcal{Y}$. This is not an ideal
solution but we found it to work well in practice. Unfortunately this
patch has the side--effect of hiding bugs in the inference algorithm,
so care is warranted.

\section{Conclusion}
\label{sec:conclusion}

We have presented a unified learning framework for reconstructing
polygonal models from single and multiple views of a scene. We have
chosen to work with the indoor Manhattan class of models in order to
leverage the parametrisation and inference algorithm recently proposed
for this hypothesis class \cite{Flint10eccv,Flint11}. Our approach to
learning performs a single optimisation with respect to a clearly
defined loss function. Experiments show our system out--performing the
state--of--the--art for multiple--view reconstruction (by a large
margin) and on one metric for single--view reconstruction.

In future work will extend this approach to learn geometry
together with scene classifiers and context--aware object detectors,
optimising with respect to a single joint loss function.

\newcommand{\CompFrame}[3]{
  \includegraphics[width=0.18\textwidth]
                  {comparison_frames/#1_frame#2_#3.png}
}

\begin{figure}[tb]%
  \centering
  \begin{tabular}{cccccc}
    \begin{sideways}
      $~~~\DepthLoss$
    \end{sideways} &
    \CompFrame{lab_foyer1}{002}{depthtrained} &
    \CompFrame{lab_ground1}{012}{depthtrained} &
    \CompFrame{exeter_mcr1}{012}{depthtrained} &
    \CompFrame{exeter_mcr1}{042}{depthtrained} &
    \CompFrame{som_corr1}{022}{depthtrained} 
    \\

    \begin{sideways}
      $~~~\LblLoss$
    \end{sideways} &
    \CompFrame{lab_foyer1}{002}{lbltrained} &
    \CompFrame{lab_ground1}{012}{lbltrained} &
    \CompFrame{exeter_mcr1}{012}{lbltrained} &
    \CompFrame{exeter_mcr1}{042}{lbltrained} &
    \CompFrame{som_corr1}{022}{lbltrained} 
    \\

    \begin{sideways}
      Flint \etal
    \end{sideways} &
    \CompFrame{lab_foyer1}{002}{iccv} &
    \CompFrame{lab_ground1}{012}{iccv} &
    \CompFrame{exeter_mcr1}{012}{iccv} &
    \CompFrame{exeter_mcr1}{042}{iccv} &
    \CompFrame{som_corr1}{022}{iccv} 
    \\

    \begin{sideways}
      $~~$Ground
    \end{sideways}
    \begin{sideways}
      $~~~$Truth
    \end{sideways} &
    \CompFrame{lab_foyer1}{002}{gt} &
    \CompFrame{lab_ground1}{012}{gt} &
    \CompFrame{exeter_mcr1}{012}{gt} &
    \CompFrame{exeter_mcr1}{042}{gt} &
    \CompFrame{som_corr1}{022}{gt} 
  \end{tabular}
  \caption{Multiple--view reconstructions predicted by our system
    (held--out samples). The first two rows represent the
    predictors trained on $\DepthLoss$ and $\LblLoss$ respectively,
    the third row is from \cite{Flint11}, and the fourth row is ground
    truth.}
  \label{fig:results-pics}
\end{figure}
